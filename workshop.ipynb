{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Workshop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" # set cuBLAS\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import *\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "\n",
    "# seed training for deterministic results\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(mode=True)\n",
    "\n",
    "# seeded generator\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previously On...\n",
    "\n",
    "During our previous workshop we saw how a multi-layer perceptron (MLP) with a ReLU non-linear activation function allowed us to learn to approximate arbitrary functions. We saw how building a deeper network with more neurons let us learn more complex and non-linear, and thus produce a more complex decision boundary.\n",
    "\n",
    "<img src=\"ReLUPiecewise.png\" alt=\"ReLU\" height=\"500\"/>\n",
    "\n",
    "We used this idea to build an MLP model to classify the arms in a spiral dataset. Our model used two inputs $P=(x_{1}, x_{2})$ and produced one output $z$ which we used to classify whether the point belonged to spiral A or B. We used $z=0$ to classify a point as belonging to spiral A and $z=0$ to classify a point as belonging to spiral B.\n",
    "\n",
    "<img src=\"SpiralClassification.png\" alt=\"Spirals\" height=\"500\"/>\n",
    "\n",
    "Within this workshop we will apply this same idea to performing image classification of digits using the MNIST dataset. We will begin with the same idea, the we can use an MLP to learn the mapping from the pixels in the image to the classified digit.\n",
    "\n",
    "<img src=\"MNISTDataset.png\" alt=\"MNIST\" height=\"300\"/>\n",
    "\n",
    "We will explore the limitations of this approach, come up with some ideas as to how we can address these issues, and build a much more performant and flexible model architecture to perform this task using convolutional neural networks.\n",
    "\n",
    "<img src=\"CNNMnist.png\" alt=\"CNN\" height=\"300\"/>\n",
    "\n",
    "We will then see how we can apply this same idea to lots of different application areas in both computer vision and pattern recognition more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Section 1] Modelling MNIST using an MLP\n",
    "\n",
    "Let's begin by seeing how we can use the ideas from the last workshop to classify images, for this we will be using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. The MNIST dataset contains a set of 70,000 images (split into 60,000 for training and 10,000 for testing) of digits and their associated labels.\n",
    "\n",
    "Each grayscale image consists of 28x28 pixels, this means we have 784 pixels $(x_{1}, ..., x_{784})$ aranged in a grid. Each label is a number which ranges from $(0-9)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Dataset\n",
    "\n",
    "As with before we will begin by loading in the dataset using `torchvision` and look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Download the MNIST dataset.\n",
    "\"\"\"\n",
    "# Download the dataset\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "mnist = MNIST(root=Path(\".\").absolute(), train=False, download=True)\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Have a look at some different examples of the images and their labels.\n",
    "\"\"\"\n",
    "# Define a function for plotting the sample\n",
    "def plot_sample(img: Image, label: int):\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"{lbl}\")\n",
    "    return fig, ax\n",
    "\n",
    "# Extract an item\n",
    "img, lbl = mnist[0] # TODO: Experiment with different numbers here\n",
    "\n",
    "# Plot the sample\n",
    "_ = plot_sample(img, lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an MLP\n",
    "\n",
    "Following our work from the previous workshop, we will use an MLP to learn the function that maps the 784 input pixels to the predicted digit in that image. We will use the MLP from the previous workshop with an input dimension of 784 and an output dimension of 10, where each output corresponds to the probably of the image being the associated digit. This is the same idea as before, however now we are learning a slightly more complex function with more input and output dimensions.\n",
    "\n",
    "We first will create a `torch.utils.dataset.Dataset` which we can use to prepare the data for the model, we will use this to flatten the image from a 2-D set of 28x28 pixels into a 1-D set of 784 pixels. We need to flatten the image in order to be able to provide it to our MLP, as we have no way to encode the pixel structure.\n",
    "\n",
    "<img src=\"Flatten.png\" alt=\"Flatten\" height=\"300\"/>\n",
    "\n",
    "We will also perform one-hot encoding of the label, where each dimension corresponds to the probability of a digit.\n",
    "\n",
    "<img src=\"OneHot.jpeg\" alt=\"OneHot\" height=\"300\"/>\n",
    "\n",
    "Alongside this we will create a `DataLoader` for splitting up the dataset.\n",
    "\n",
    "Have a go at creating the `MLP` using the previously discussed number of input and output dimensions, feel free to decide how many hidden layers and dimensions you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a dataset to load and preparing samples for our MLP\n",
    "\"\"\"\n",
    "class OurMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, mnist: MNIST, num_samples: Optional[int] = None, flatten: Optional[bool] = True):\n",
    "        super(OurMNIST, self).__init__()\n",
    "        self.mnist = mnist\n",
    "        self.num_samples = num_samples if num_samples is not None else len(self.mnist)\n",
    "        self.index = list(range(self.num_samples))\n",
    "        assert len(self.index) < len(self.mnist), f\"Tried to use {num_samples} but MNIST only has {len(self.minst)}\"\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
    "        img, lbl = self.mnist[index] # load from mnist\n",
    "\n",
    "        # image prep\n",
    "        img = torch.from_numpy(np.asarray(img).copy()) # copy into tensor\n",
    "        if self.flatten: \n",
    "            img = img.reshape(28*28) # flatten from 28x28 to 784\n",
    "        else:\n",
    "            img = img.unsqueeze(0) # using img shape as [1,28,28]\n",
    "        img = img.to(dtype=torch.float32) # cast to fp32\n",
    "        img = img / 255 # scale to 0-1\n",
    "\n",
    "        # label prep\n",
    "        lbls = torch.zeros((10), dtype=torch.float32) # create one-hot encoding of vector\n",
    "        lbls[lbl] = 1.0 # set label index\n",
    "\n",
    "        return img, lbls\n",
    "\n",
    "# Create the dataset\n",
    "# We will only use a subset of the samples to make sure we can train the models in a reasonable amount of time\n",
    "dataset = OurMNIST(mnist, num_samples=1000, flatten=True)\n",
    "\n",
    "# Show the dimensions of one of the samples\n",
    "img, lbl = dataset[15]\n",
    "print(f\"Flattened image has the shape {img.shape}: {img}\")\n",
    "print(f\"One-hot encoded label has the shape {lbl.shape}: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create the dataloaders to load data during training\n",
    "\"\"\"\n",
    "# Split up the dataset into training and testing \n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# Create dataloaders to create batches from the datasets\n",
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, generator=g)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, generator=g)\n",
    "\n",
    "# Print some information\n",
    "print(f\"Training DataLoader: {len(train_dataloader)} batches with {batch_size} samples/batch\")\n",
    "print(f\"Validation DataLoader: {len(valid_dataloader)} batches with {1} sample/batch\")\n",
    "print(f\"Testing DataLoader: {len(test_dataloader)} batches with {1} sample/batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets define an model architecture\n",
    "\"\"\"\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\" Create a single-layer perceptron with ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int, dropout: Optional[float] = None, use_activation: Optional[bool] = True):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim) # linear mapping y = Wx + B\n",
    "        self.act = nn.ReLU() if use_activation else None # non-linear activation function\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout is not None else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc(x)) if self.act is not None else self.fc(x)\n",
    "        x = self.dropout(x) if self.dropout is not None else x\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\" Create a multi-layer perceptron by stacking multiple layers of perceptrons.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_layers: int, hidden_dim: int):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        # Create the layers\n",
    "        modules = [Perceptron(input_dim, hidden_dim)] # input layer\n",
    "        for _ in range(hidden_layers):\n",
    "            modules.append(Perceptron(hidden_dim, hidden_dim)) # hidden layer/s\n",
    "        modules.append(Perceptron(hidden_dim, output_dim, use_activation=False)) # output layer\n",
    "\n",
    "        # Create the model\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x, lbl):\n",
    "        lbl = torch.argmax(lbl)\n",
    "        lbl_p = torch.argmax(self.forward(x))\n",
    "        img = x.reshape(28,28)\n",
    "        return img, lbl, lbl_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = MultiLayerPerceptron( # TODO: Define the number of input, output and hidden dimensions\n",
    "    input_dim=784,\n",
    "    output_dim=10,\n",
    "    hidden_layers=2,\n",
    "    hidden_dim=16\n",
    ")\n",
    "\n",
    "# Display the model\n",
    "print(model)\n",
    "print(f\"Our model has {sum([np.prod(p.shape) for p in model.layers[0].parameters() if p.requires_grad])} learnable parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note just how many parameters our model has, whilst multi-layer perceptrons can be used for classification their architecture is generally impractice for larger resolution inputs e.g. consider an ML for a 1024x1024 image with an embedding dimensions of 1024, this would require 3.22B parameters in the first layer alone. There are more effective ways to use these weights as we will see..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Model\n",
    "\n",
    "We will use the same setup as in the previous workshop, we will create a training loop where we show the model examples of images and then optimize the networks parameters slights, and we will also create a testing loop where we show the model examples of images it hasn't seen during training to evaluate it's performance.\n",
    "\n",
    "We'll then create all the components we need and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create the training loop\n",
    "\"\"\"\n",
    "def train_epoch(dataloader: Callable, model: Callable, loss_fn: Callable, opt: Callable, device: Any) -> Tuple[List[float], Callable]:\n",
    "    \"\"\" Perform a training epoch.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for idx, (img, lbl) in enumerate(dataloader):\n",
    "        # move data to device\n",
    "        img = img.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "\n",
    "        # forwards pass\n",
    "        lbl_p = model(img)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(lbl_p, lbl)\n",
    "\n",
    "        # backwards pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step and clear gradients\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # store loss\n",
    "        losses.append(loss.detach().cpu())\n",
    "\n",
    "    losses = torch.stack(losses, dim=0)\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "\n",
    "def valid_epoch(dataloader: Callable, model: Callable, loss_fn: Callable, device: Any) -> Tuple[List[float], Callable]:\n",
    "    \"\"\" Perform a validation/testing epoch.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, lbl) in enumerate(dataloader):\n",
    "            # move data to device\n",
    "            img = img.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "\n",
    "            # forwards pass\n",
    "            lbl_p = model(img)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = loss_fn(lbl_p, lbl)\n",
    "\n",
    "            # store loss\n",
    "            losses.append(loss.detach().cpu())\n",
    "\n",
    "    losses = torch.stack(losses, dim=0)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def test_epoch(dataloader: Callable, model: Callable, device: Any) -> Tuple[List[float], Callable]:\n",
    "    \"\"\" Perform a validation/testing epoch.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    labels_p = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, lbl) in enumerate(dataloader):\n",
    "            # move data to device\n",
    "            img = img.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "\n",
    "            # forwards pass\n",
    "            img, lbl, lbl_p = model.predict(img, lbl)\n",
    "\n",
    "            # store loss\n",
    "            images.append(img.detach().cpu())\n",
    "            labels.append(lbl.detach().cpu())\n",
    "            labels_p.append(lbl_p.detach().cpu())\n",
    "\n",
    "    # concatenate results\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    labels_p = torch.stack(labels_p, dim=0)\n",
    "\n",
    "    return images, labels, labels_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create the training loop and train our model!\n",
    "\"\"\"\n",
    "def train_model(model, train_dl: Callable, valid_dl: Callable, test_dl: Callable, loss_fn: Callable, opt: Callable, device: Any, epochs: int):\n",
    "    # Prepare\n",
    "    train_losses = []\n",
    "    valid_losses = {}\n",
    "    train_loss, valid_loss = .0, .0\n",
    "\n",
    "    # Training Loop\n",
    "    with tqdm(range(epochs)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            # Training Epoch\n",
    "            pbar.set_description(f\"[TRAIN] Epoch: {epoch+1}/{epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "            model, losses = train_epoch(train_dl, model, loss_fn, opt, device)\n",
    "            train_loss = sum(losses) / len(losses)\n",
    "            train_losses += losses.numpy().tolist()\n",
    "            pbar.set_description(f\"[TRAIN] Epoch: {epoch+1}/{epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "            # Validation Epoch\n",
    "            pbar.set_description(f\"[VALID] Epoch: {epoch+1}/{epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "            losses = valid_epoch(valid_dl, model, loss_fn, device)\n",
    "            valid_loss = sum(losses) / len(losses)\n",
    "            valid_losses[(epoch+1) * len(train_dl)] = valid_loss\n",
    "            pbar.set_description(f\"[VALID] Epoch: {epoch+1}/{epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Testing Epoch at end of training\n",
    "    pbar.set_description(f\"[TESTING] Epoch: {epoch+1}/{epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    test_images, test_labels, test_labels_p = test_epoch(test_dl, model, device)\n",
    "    pbar.set_description(f\"[TESTING] Epoch: {epoch+1}/{epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    return model, train_losses, valid_losses, test_images, test_labels, test_labels_p\n",
    "\n",
    "\n",
    "# Setup training items\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_layers=2, hidden_dim=16).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Perform training and testing\n",
    "model, train_losses, valid_losses, test_images, test_labels, test_labels_p = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    valid_dataloader, \n",
    "    test_dataloader, \n",
    "    loss_fn, \n",
    "    opt, \n",
    "    device, \n",
    "    30 # TODO: Feel free to change the number of epochs you want to train the model for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the training dynamics\n",
    "\"\"\"\n",
    "# Plot losses from training\n",
    "def plot_training_performance(train_losses, valid_losses):\n",
    "  fig = plt.figure(figsize=(16,4))\n",
    "  ax = fig.add_subplot(111)\n",
    "  ax.plot(train_losses, linestyle=\"-\", color=\"tab:blue\", label=\"Train\", alpha=0.75)\n",
    "  ax.plot(valid_losses.keys(), valid_losses.values(), linestyle=\"-\", color=\"tab:orange\", label=\"Valid\", alpha=0.75)\n",
    "  ax.set_xlim(left=0, right=len(train_losses))\n",
    "  ax.set_ylim(bottom=0)\n",
    "  ax.set_xlabel(\"Iterations\")\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "  ax.set_title(\"Training Dynamics\")\n",
    "  ax.legend(loc=\"best\")\n",
    "  ax.grid(True, alpha=0.25)\n",
    "  \n",
    "\n",
    "# Plot \n",
    "plot_training_performance(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the model performance\n",
    "def plot_prediction(img: Tensor, lbl: int, lbl_p: int) -> Tuple[Any, Any]:\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"Ground-truth: {lbl} vs. Predicted: {lbl_p}\")\n",
    "\n",
    "\n",
    "# Plot predicted\n",
    "num_plot = 2 # TODO: Explore different samples by re-running this cell\n",
    "indexes = random.sample(list(range(len(test_images))), num_plot)\n",
    "for index in indexes:\n",
    "    plot_prediction(test_images[index], test_labels[index], test_labels_p[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Compute the accuracy of our model.\n",
    "\"\"\" \n",
    "n_correct = torch.where(test_labels == test_labels_p)[0].shape[0]\n",
    "n_total = test_labels.shape[0]\n",
    "print(f\"{100*n_correct/n_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining our Model\n",
    "\n",
    "Depending on how we design our model we can generally perform digit classification reasonably well. Let's look into our model at the different weights and biases to try and see what it's learned and how it's performing the digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the network parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "ax.imshow(torch.clone(model.layers[0].fc.weight).detach().cpu().numpy(), cmap=\"RdYlBu\", aspect=\"auto\")\n",
    "ax.set_title(\"Weights for Perceptron 0\")\n",
    "ax.set_ylabel(\"Outputs [Hidden Dimension]\")\n",
    "ax.set_xlabel(\"Inputs [Input Dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Weights\n",
    "perceptron_num = 8 # TODO: Channel 0 corresponds to perceptron 0\n",
    "weights = torch.clone(model.layers[0].fc.weight).detach().cpu()[perceptron_num].reshape(28,28)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.imshow(weights, cmap=\"RdYlBu\", aspect=\"auto\")\n",
    "ax.set_title(f\"[Perceptron {perceptron_num}] Weights in Image Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that just looking at the raw weights and biases is generally not the most interpretable thing in the world - one of the issues with deep learning. However, we can evaluate the model on some examples and look at the activations and try to build up some intuition around how the network is learning to map these 784 inputs to the 10 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Look at the network activations on some dataset examples\n",
    "\"\"\"\n",
    "# Plot the module activations\n",
    "def create_hook_function(name: str):\n",
    "    def plot_module_activations(module, inp, out):\n",
    "        out = out.detach().cpu().unsqueeze(0)\n",
    "        fig, ax = plt.subplots(figsize=(16,1))\n",
    "        ax.imshow(out, cmap=\"RdYlBu\", aspect=\"auto\")\n",
    "        ax.set_xticks(np.arange(out.shape[-1]))\n",
    "        # ax.grid(True, alpha=0.25)\n",
    "        ax.set_title(f\"{name}\")\n",
    "    return plot_module_activations\n",
    "\n",
    "\n",
    "# Attach plotting function to module\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    if hasattr(layer, \"_forward_hooks\"):\n",
    "        layer.fc._forward_hooks.clear()\n",
    "    layer.fc.register_forward_hook(create_hook_function(f\"Layer {idx}: {layer}\"))\n",
    "\n",
    "\n",
    "# Perform forwards pass and plot results\n",
    "index = 0 # TODO: See if you can find a pattern in the activations.\n",
    "img, lbl = test_dataset[index]\n",
    "img = img.to(device)\n",
    "img, lbl, lbl_p = model.predict(img, lbl)\n",
    "plot_prediction(img.cpu(), lbl, lbl_p)\n",
    "\n",
    "\n",
    "# Clear up the hooks\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    if hasattr(layer, \"_forward_hooks\"):\n",
    "        layer.fc._forward_hooks.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking our Model\n",
    "\n",
    "We have created a MLP model that is capable of fitting the dataset we have trained it on. Similarly to the spiral dataset, when we evaluate the model within the distribution seen during training we can generally expect similar performance. However, we can explore whether the model has managed to generalize the rules necessary for classifying digits.\n",
    "\n",
    "Let's explore what happens when we translate the image slightly to the left or the right, given the model was never trained on digits in this position - let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Apply different translations to the image.\n",
    "\"\"\"\n",
    "# Load image from dataset\n",
    "img, lbl = test_dataset[0] # TODO: Explore different samples\n",
    "\n",
    "# Transform the image using an affine transform\n",
    "img = img.reshape(28,28).unsqueeze(0)\n",
    "img = torchvision.transforms.functional.affine(img, angle=0.0, translate=[0, 0], scale=1.0, shear=0.0) # TODO: Explore different affine transforms\n",
    "img = img[0].reshape(28*28)\n",
    "\n",
    "# Pass image to model for prediction\n",
    "img = img.to(device)\n",
    "img, lbl, lbl_p = model.predict(img, lbl)\n",
    "img = img.cpu()\n",
    "plot_prediction(img, lbl, lbl_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore what happens if we apply different transformations to the image such as rotation or scaling. As before, our model has not seen these sorts of arangements of pixels - let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the cell above try some different rotations, translations, scaling, and shearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We can see that the performance on these arangements of pixels significantly degrades the performance of the network, it's clear that our model has not learned to generalize the features required for recognizing digits. \n",
    "\n",
    "Multi-layer perceptrons operate flattened inputs, as a result it operates on the entire image at once, it doesn't have an awareness of the spatial structure of the image. In this sense it's dependent on the absolute position of the pixels, if we rotate the image it changes the flattened input completely.\n",
    "\n",
    "Whilst it work, it does not inherently provide many nice properties such as translation invariance, and is generally quite parameter dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra Example 1**\n",
    "\n",
    "However, an interesting tidbit from this is that we can randomly permute the inputs so the image looks completely garbled when we visualize it. However, the MLP is still able to learn the relationship between the different pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly index the pixel positions\n",
    "...\n",
    "\n",
    "# Visualize the image\n",
    "...\n",
    "\n",
    "# Train a model\n",
    "...\n",
    "\n",
    "# Evaluate the performance\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra Example 2**\n",
    "During out previous workshop we discuss the balance between the dataset complexity and the model performance. We can take another look at our testing results and break the performance metrics down per digit - we can see the model performs differently on these different classes. We can relate this to the internal balance between intra-class and inter-class complexity, as the model has a fixed capacity each class is competing for resources, some classes require less complex functions to learn to represent than others, as a result the model performs differently on the different classes.\n",
    "\n",
    "(plot a 1 vs. a 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the testing performance per class\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Section 2] Detecting Edges by Convolving Filters\n",
    "\n",
    "Let's think about how we try to classify these numbers, why would you define this digit as a 5?\n",
    "\n",
    "<img src=\"MNIST_5.png\" alt=\"5\" height=\"300\"/>\n",
    "\n",
    "How about a 7?\n",
    "\n",
    "<img src=\"MNIST_7.png\" alt=\"7\" height=\"300\"/>\n",
    "\n",
    "When we classify digits we tend to be looking for different spatial features such as lines and curves in a spatially equivariant manner (e.g. a line is a line no matter where it is in the image), we're then combining these lower order features into more contextual and discriminative feature space. Let's think about how we can do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters\n",
    "\n",
    "We want some sort of process we can apply to images which will allow us to extract spatial features such as lines and curves in a spatially equivariant manner.\n",
    "\n",
    "We will first take a look a long employed techniques in image process and computer vision called filters - these filters are manually designed matrices we can convolve over images to achieve results such as sharpening, image blurring or edge detection.\n",
    "\n",
    "<img src=\"GaussianKernel.png\" alt=\"Gaussian\" height=\"300\"/>\n",
    "\n",
    "<img src=\"GaussianBlurred.png\" alt=\"Blur\" height=\"300\"/>\n",
    "\n",
    "We've seen how these work during the lectures, we convolve these matices (or more commonly referred to as kernels) over the image. For a given point they comput the weighted sums of the pixels within the region defined by the kernel.\n",
    "\n",
    "<img src=\"Convolution.gif\" alt=\"Convolution\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Convolving Filters\n",
    "\n",
    "We can play around with these kernels ourselve to gain some intuition as to how they work.\n",
    "\n",
    "We'll begin by defining a 3x3 filter which defines weights for different positions of the kernel, you can make a direct comparison from these kernel to appling an MLP over a smaller region of the image. Where our kernel (or MLP) is now just learning much smaller and simpler features.\n",
    "\n",
    "<img src=\"KernelExamples.png\" alt=\"Kernels\" height=\"300\"/>\n",
    "\n",
    "We will take a kernel and apply it to each position in the input image, effectively sliding or convolving, the kernel over the input image. We can see here by sliding these kernels over the image we obtain a strong positive activations for left edges and strong negative activations for right edges. Importantly by applying the same filter everywhere in the image we can extract edges no matter where they are in the image, our result is translation equivariant.\n",
    "\n",
    "Let's explore some different filter designs and see what sort of spatial features we can extract..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lower horitonztal edge filter\n",
    "our_filter_weights = torch.tensor([\n",
    "    [-1.0, -1.0, -1.0],\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [0.0, 0.0, 0.0]\n",
    "    ], dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Define a right vertical edge filter\n",
    "# our_filter_weights = torch.tensor([ # TODO: Uncomment this if you want to try the vertical edge filter\n",
    "#     [-1.0, 1.0, 0.0],\n",
    "#     [-1.0, 1.0, 0.0],\n",
    "#     [-1.0, 1.0, 0.0]\n",
    "#     ], dtype=torch.float32\n",
    "# )\n",
    "\n",
    "# Let's plot visualize the filter\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "ax.imshow(our_filter_weights, cmap=\"RdYlBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's package it up in a `Conv2d` module for simplcity\n",
    "conv_filter = nn.Conv2d(1, 1, kernel_size=(3,3), padding=1, bias=False)\n",
    "conv_filter.weight = nn.Parameter(our_filter_weights.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "\n",
    "# Convolve the filter over an image\n",
    "img, lbl = test_dataset[0] # TODO: Try out different numbers\n",
    "img = img.reshape(28,28)\n",
    "filtered_img = conv_filter(img.unsqueeze(0)).detach().squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "# Visualize the filtered image\n",
    "fig, ax = plt.subplots(figsize=(12,6), ncols=2)\n",
    "ax[0].imshow(img.squeeze(0), cmap=\"gray\")\n",
    "ax[1].imshow(filtered_img, cmap=\"RdYlBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building up Abstraction: Building a Multi-layer CNN\n",
    "\n",
    "Similarly to our previous workshop, using a single kernel does not allow us to do much. However, the power comes when we start to use multiple of the kernels in a layer to extract numerous spatial features at once. Furthermore, we can stack layers of these kernels to extract more complex and abstract spatial features.\n",
    "\n",
    "As we increase kernels in our first layer we allow our model to learn to extract more varied sets of edges from the input image. As we begin to add additional layers of filters allow our model to learn non-linear combinations of the previous filters, allowing us to extract higher-order and increasingly abstract features such as lines, curves, shapes and textures. This ultimately allows us to recognize specific spatial patterns which allow us to perform image classification.\n",
    "\n",
    "Hand-crafting these filters rapidly becomes an arduous task, especially if we consider adding multiple layers of filters, the sorts of filters and features we want to extract become extremely nebulous. Thankfully, we can use back-propagation and gradient descent based optimization to learn these filters in our network automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a CNN\n",
    "\n",
    "We will first define a single convolutional layer `ConvLayer`, this layer has `out_channels` kernels associated with it - allowing it to learn `out_channels` sets of filters. An important part of this layer is the `MaxPool2d` which performs pooling in a 3x3 kernel which reduces the dimension of the extracted features (compressing information) and helps provide local translation invariance.\n",
    "\n",
    "We can then define a model `MultiLayerConv` which stacks multiple of these layers, this allows us to extract progressively more abstract spatial features. We use a `MultiLayerPerceptron` as the final layer of our network to learn the mapping from our extracted spatial features to our 10-dimensional output, each of which represent the probability of the image being classified as a given digit.\n",
    "\n",
    "Let's see how it performs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets define a single Conv layer\n",
    "\"\"\"\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\" Create a single-layer convolution module with ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        # Convolution layer\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1, bias=True)\n",
    "\n",
    "        # Non-linear ReLU activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Max pooling to reduce dimensionality\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2), padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.conv(x)) if self.act is not None else self.conv(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let's define a model using multiple Conv layers\n",
    "\"\"\"\n",
    "class MultiLayerConv(nn.Module):\n",
    "    \"\"\" Create a multi-layer convolutional network by stacking multiple convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers: int, hidden_dim: int):\n",
    "        super(MultiLayerConv, self).__init__()\n",
    "        \n",
    "        # Create an input layer\n",
    "        modules = [ConvLayer(1, hidden_dim)] # input layer\n",
    "\n",
    "        # Create some hidden layers\n",
    "        for _ in range(hidden_layers):\n",
    "            modules.append(ConvLayer(hidden_dim, hidden_dim)) # hidden layer/s\n",
    "\n",
    "        # Add these to the model\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "\n",
    "        # Create a fully connected output layer\n",
    "        self.fc = Perceptron(hidden_dim*(2**2), 10, use_activation=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # extract spatial features using conv layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # flatten & predict logits\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C*H*W)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x, lbl):\n",
    "        lbl = torch.argmax(lbl)\n",
    "        lbl_p = torch.argmax(self.forward(x))\n",
    "        return x, lbl, lbl_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets print out what we've created\n",
    "\"\"\"\n",
    "# Model Parameters\n",
    "hidden_layers = 2\n",
    "hidden_dim = 16\n",
    "\n",
    "# Create simple CNN\n",
    "model = MultiLayerConv(hidden_layers, hidden_dim)\n",
    "\n",
    "# Display the model\n",
    "print(model)\n",
    "print(f\"Our model has {sum([np.prod(p.shape) for p in model.parameters() if p.requires_grad])} learnable parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note here is how much less parameters our model has than our original densely connected network, convolution networks exploit weight sharing by applying the same kernel across the entire image - drastically reducing the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training items\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = MultiLayerConv(hidden_layers, hidden_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = OurMNIST(mnist, num_samples=1000, flatten=False)\n",
    "\n",
    "# Split up the dataset into training and testing \n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# Create dataloaders to create batches from the datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, generator=g)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, generator=g)\n",
    "\n",
    "# Perform training and testing\n",
    "model, train_losses, valid_losses, test_images, test_labels, test_labels_p = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    valid_dataloader, \n",
    "    test_dataloader, \n",
    "    loss_fn, \n",
    "    opt, \n",
    "    device, \n",
    "    30 # TODO: Feel free to change the number of epochs you want to train the model for\n",
    ")\n",
    "\n",
    "# Plot \n",
    "plot_training_performance(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted\n",
    "num_plot = 2 # TODO: Explore different samples by re-running this cell\n",
    "indexes = random.sample(list(range(len(test_images))), num_plot)\n",
    "for index in indexes:\n",
    "    plot_prediction(test_images[index].squeeze(0,1), test_labels[index], test_labels_p[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Compute the accuracy of our model.\n",
    "\"\"\" \n",
    "n_correct = torch.where(test_labels == test_labels_p)[0].shape[0]\n",
    "n_total = test_labels.shape[0]\n",
    "print(f\"{100*n_correct/n_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter & Feature Visualization\n",
    "\n",
    "We can look at some of the different filters the model has learned during training see if our intuition has any merit. \n",
    "\n",
    "As we get deeper into the network looking at the filters themselves.becomes less helpful as they depend on all of the previous filters, as a result we can instead look at features to see what areas in the input image activate this filter to better understand what it represents. We do this by observing the output activations from these different features to see how the network is progressively extracting increasingly abstract spatial features from the input image.\n",
    "\n",
    "*What do you expect to see?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let's explore some of the early-scale kernels\n",
    "\"\"\"\n",
    "# Extract and display early-scale kernel\n",
    "kernel_index = 1 # TODO: Try some numbers from 1 to hidden_dim\n",
    "kernel = torch.clone(model.layers[0].conv.weight[kernel_index,0]).cpu().detach()\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "ax.imshow(kernel, cmap=\"RdYlBu\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let's see what features are extracted from an image using this kernel\n",
    "\"\"\"\n",
    "# Let's package it up in a `Conv2d` module for simplcity\n",
    "conv_filter = nn.Conv2d(1, 1, kernel_size=(3,3), padding=1, bias=False)\n",
    "conv_filter.weight = nn.Parameter(kernel.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "# Convolve the filter over an image\n",
    "img, lbl = test_dataset[0] # TODO: Try out different numbers\n",
    "filtered_img = conv_filter(img.unsqueeze(0)).detach().squeeze(0).squeeze(0)\n",
    "\n",
    "# Visualize the filtered image\n",
    "fig, ax = plt.subplots(figsize=(8,4), ncols=2)\n",
    "ax[0].imshow(img.squeeze(0), cmap=\"gray\", aspect=\"auto\")\n",
    "ax[1].imshow(filtered_img, cmap=\"RdYlBu\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore looking at some deeper features, however their meaning becomes less clear as the features generally become more abstract... To extract more useful insights and visualization we generally lend on other interpretability tooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let's look at some of the features extracted by deeper kernels\n",
    "\"\"\"\n",
    "# Plot the module activations\n",
    "def create_hook_function(name: str, index: int):\n",
    "    def plot_module_activations(module, inp, out):\n",
    "        out = out.detach().cpu()\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ax.imshow(out[0,index], cmap=\"RdYlBu\", aspect=\"auto\")\n",
    "        ax.set_xticks(np.arange(out.shape[-1]))\n",
    "        ax.set_title(name)\n",
    "    return plot_module_activations\n",
    "\n",
    "\n",
    "# Attach plotting function to module\n",
    "layer_index = 0\n",
    "kernel_index = 0\n",
    "module = model.layers[layer_index].conv\n",
    "if hasattr(module, \"_forward_hooks\"):\n",
    "    module._forward_hooks.clear()\n",
    "module.register_forward_hook(create_hook_function(f\"Feature Map {kernel_index} from Layer {layer_index}\", kernel_index))\n",
    "\n",
    "\n",
    "# Perform forwards pass and plot results\n",
    "img, lbl = test_dataset[0] # TODO: Try out some different samples\n",
    "img = img.to(device).unsqueeze(0)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.imshow(img.cpu().squeeze(0,1), cmap=\"gray\")\n",
    "ax.set_title(f\"Input Image\")\n",
    "_ = model(img)\n",
    "\n",
    "\n",
    "# Clear up the hooks\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    module = layer.conv\n",
    "    if hasattr(module, \"_forward_hooks\"):\n",
    "        module._forward_hooks.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in feature visualization or more general interpretability tools for deep-learning models there is an excellent article on [DistillPub - Feature Visualization](https://distill.pub/2017/feature-visualization/) and [Captum](https://github.com/pytorch/captum) has some excellent tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking our Model\n",
    "\n",
    "Our previous MLP model was extremely sensitive the translation, our new CNN model should be able to better handle translations within the image plane. However, due to the multi-layer perceptron and the maximum pooling, our model will not truly be translation invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Apply different translations to the image...\n",
    "\"\"\"\n",
    "# Load image from dataset\n",
    "img, lbl = test_dataset[0] # TODO: Explore different samples\n",
    "\n",
    "# Transform the image using an affine transform\n",
    "img = torchvision.transforms.functional.affine(img, angle=0.0, translate=[0, 0], scale=1.0, shear=0.0) # TODO: Explore different translations\n",
    "\n",
    "# Pass image to model for prediction\n",
    "img = img.unsqueeze(0).to(device)\n",
    "img, lbl, lbl_p = model.predict(img, lbl)\n",
    "img = img.cpu().squeeze(0,1)\n",
    "plot_prediction(img, lbl, lbl_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to have improved robustness to the pertubations in positions. However more interestingly is that we've managed to achieve this alongside a performance improvement whilst using significantly less parameters than our previous MLP model.\n",
    "\n",
    "Being able to design neural networks which exploit the underlying symmetries of your problem, in the case the natural spatial structure of our image, is a very fundamental and powerful design philosophy in deep learning.\n",
    "\n",
    "Out of curiousity let's see how our model reacts to rotations and scaling of the image...\n",
    "\n",
    "*What do you expect?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Rotate the image...\n",
    "\"\"\"\n",
    "# TODO: In the previous cell try rotating the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Scale the image...\n",
    "\"\"\"\n",
    "# TODO: In the previous cell try rotating the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so our model seems to struggle with rotation and scaling, lets have a think about why this occurs.\n",
    "\n",
    "*How can we go about solving this?*\n",
    "\n",
    "Our model architecture is perfectly capable of learning rotated or even upside down digits through the kernels, these would just represent a different distribution of spatial features and thus filters for our model to learn. \n",
    "\n",
    "During our previous workshop we showed that if we made the dataset more complex we had to increase the complexity of the model to match this, the same idea also applies here. Lets say we want to train our model on straight AND rotated images, then our network is going to have to (potentially) allocate some additional capacity to learn these other filters for different angles and orientations, and combinations of these edges that it didn't have to before - it has more to learn. \n",
    "\n",
    "We could also alter our model architecture such by incorporating rotation invariance into the architecture itself, but it turns out to be much more computationally efficient and practical to simply increase the scale of our model as use these same building blocks, whilst simply augmenting the training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a dataset with augmentations\n",
    "\"\"\"\n",
    "class AugmentedOurMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, mnist: MNIST, num_samples: Optional[int] = None, flatten: Optional[bool] = True):\n",
    "        super(AugmentedOurMNIST, self).__init__()\n",
    "        self.mnist = mnist\n",
    "        self.num_samples = num_samples if num_samples is not None else len(self.mnist)\n",
    "        self.index = list(range(self.num_samples))\n",
    "        assert len(self.index) < len(self.mnist), f\"Tried to use {num_samples} but MNIST only has {len(self.minst)}\"\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
    "        img, lbl = self.mnist[index] # load from mnist\n",
    "\n",
    "        # image prep\n",
    "        img = torch.from_numpy(np.asarray(img).copy()) # copy into tensor\n",
    "        if self.flatten: \n",
    "            img = img.reshape(28*28) # flatten from 28x28 to 784\n",
    "        else:\n",
    "            img = img.unsqueeze(0) # using img shape as [1,28,28]\n",
    "            \n",
    "        # TODO: You can apply these transforms randomly to \"augment\" the training distribution\n",
    "        prob_of_applying = random.random()\n",
    "        if prob_of_applying > 0:\n",
    "            img = torchvision.transforms.functional.affine(img, angle=180.0, translate=[0, 0], scale=1.0, shear=0.0)\n",
    "\n",
    "        img = img.to(dtype=torch.float32) # cast to fp32\n",
    "        img = img / 255 # scale to 0-1\n",
    "\n",
    "        # label prep\n",
    "        lbls = torch.zeros((10), dtype=torch.float32) # create one-hot encoding of vector\n",
    "        lbls[lbl] = 1.0 # set label index\n",
    "\n",
    "        return img, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training items\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = MultiLayerConv(hidden_layers, hidden_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AugmentedOurMNIST(mnist, num_samples=1000, flatten=False)\n",
    "\n",
    "# Split up the dataset into training and testing \n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# Create dataloaders to create batches from the datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, generator=g)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, generator=g)\n",
    "\n",
    "# Perform training and testing\n",
    "model, train_losses, valid_losses, test_images, test_labels, test_labels_p = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    valid_dataloader, \n",
    "    test_dataloader, \n",
    "    loss_fn, \n",
    "    opt, \n",
    "    device, \n",
    "    30\n",
    ")\n",
    "\n",
    "# Plot \n",
    "plot_training_performance(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you apply it randomly you may need to increase the training time or model to suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted\n",
    "num_plot = 2\n",
    "indexes = random.sample(list(range(len(test_images))), num_plot)\n",
    "for index in indexes:\n",
    "    plot_prediction(test_images[index].squeeze(0,1), test_labels[index], test_labels_p[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Section 3] Pre-trained Models\n",
    "\n",
    "Whilst we can train these models ourselves, sometimes it's extremely useful to be able to piggyback off a pre-existing solution. In these scenarios pre-trained models are extremely useful, rather than having to go through the R&D of designing a model, collecting enough data, and figuring our how to train a model - we can use a model someone has already trained.\n",
    "\n",
    "However there are some important caveats with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are pre-trained models?\n",
    "\n",
    "Pre-trained models involve a model that someone has trained on a dataset to perform a specific task such as object detection, image classification, 3D landmark detection, image generation, etc. We can use these pre-trained models to perform different tasks.\n",
    "\n",
    "The following links contain some good resources.\n",
    "- https://pytorch.org/vision/stable/models.html (computer vision models)\n",
    "- https://huggingface.co/blog/cv_state (much more complex models and pre-built pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "depth_estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "output = depth_estimator(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n",
    "\n",
    "# This is a tensor with the values being the depth expressed\n",
    "# in meters for each pixel\n",
    "output[\"depth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use pre-trained models?\n",
    "\n",
    "Similarly to how our model trained on digits really only knows how to classify digits, we need to be aware of what the different pre-trained models are pre-trained to do. E.g. there's not point using a model for image classification if you want to perform object detection out-of-the-box.\n",
    "\n",
    "Furthermore, as we saw our CNN was reasonable on in-distribution samples, however when we rotated the image slightly it fell over. When using pre-trained models we have to be aware of the training distribution to understand in what situations we can expect reasonable performance.\n",
    "\n",
    "However, most of the time they can just be extremely fun to play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers[\"torch\"] transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "generator = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "generator.to(\"cuda:0\")\n",
    "\n",
    "image = generator(\"An image of a squirrel in Picasso style\").images[0]\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = generator(\"Student Studying Machine Learning on a Laptop\").images[0]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning pre-trained models?\n",
    "\n",
    "Across various computer vision tasks such as object detection or depth estimation, much of what we'd expect the early layers to do in these networks remains the same e.g. learning edges, shapes, and textures. Pre-trained models generally have quite good feature extraction.\n",
    "\n",
    "Say we have a fairly small scale dataset that we want to train a model on e.g. to detect specific types of objects. We can leverage the pre-trained features in these models to expedite the learning process. We can either re-train a model, you can think of this as starting training but from a much better initialization point than just random weights and biases. \n",
    "\n",
    "Sometimes this can involve modify the architecture slightly, such as just the classification head to use a different number of classes, whilst retaining the backbone.\n",
    "\n",
    "We can also freeze part of our model to avoid modifying the existing components, and just learn the higher order features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/stable/models.html\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Freeze the network parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Overwrite the classification head of the network\n",
    "your_number_of_classes = 16\n",
    "model.fc = nn.Linear(2048, your_number_of_classes)\n",
    "\n",
    "# Run training for just the classification head\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are foundation models?\n",
    "Well they're really not different from pre-trained models, however the scale and complexity both of the model and dataset is typically much larger - we hope this results in much more generalized features to the point where fine-tuning is not required for some down-stream task aka. they have strong zero-shot performance. We can use the much more robust and representative features from these models to perform different tasks without as much re-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "!pip install -q supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from pathlib import Path\n",
    "\n",
    "# Load model\n",
    "checkpoint = Path(\".\").joinpath(\"sam_vit_b_01ec64.pth\")\n",
    "model_type = \"vit_b\"\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pre-existing example image\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
    "import cv2\n",
    "image_bgr = cv2.imread(\"dog.jpeg\")\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Perform some mask generation around an object\n",
    "\"\"\" \n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "# Disable deterministic\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "# Create mask generator\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    pred_iou_thresh=0.95,\n",
    "    stability_score_thresh=0.95\n",
    ")\n",
    "\n",
    "# Run forwards process on image\n",
    "sam_results = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "detections = sv.Detections.from_sam(sam_result=sam_results)\n",
    "print(detections.mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img = image_rgb.copy()\n",
    "_img[~detections.mask[0]] = .0\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.imshow(_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out: https://github.com/RockeyCoss/Prompt-Segment-Anything?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra Example**\n",
    "Lots of different advancements in different applications of computer vision just come from thinking about how these general architectures can be applied in different ways, and finding the data to do so. Rather than just performing object classification - take in 28*28 pixels and output 0-9 numbers.\n",
    "\n",
    "We could equally use this architecture to detect 2D facial landmarks, where we input say 72*72 pixel image, and output 68*2 numbers representing the position of some points on the face.\n",
    "\n",
    "We could output 72*72*Nc numbers, re-mapping back to the image where each pixel is associated with a class probability - we call this semantic segmentation.\n",
    "\n",
    "We could also outptu a number with a 72*72 where pixel has a number associated with it, this is called depth estimation\n",
    "\n",
    "Lots of different applications of computer vision exist, each of these different problems has a different set of application specific challenges related to it, and will have their own data structures and dataset used for training, but we can use these building blocks and put them together in differents ways e.g. CNNs to do these different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra Example**\n",
    "Different types of architectures generally has these different intuitions built into them, these are called inductive biases, they help the models be much more efficient in their learning because they're biases towards exploiting a symmetry that we've designed. Understanding what the intuition behind these models and how they function is a core aspect of designing machine learning systems - \n",
    "\n",
    "But that's not to say you can't use these in the wrong way - you can equally express the image as a sequence of 784 pixels - and while it's not ideal you can equally train an LSTM to predict MNIST images in this manner.\n",
    "\n",
    "However the LSTM doesn't exploit the structure of images in a very natural way - but say we had some sequential data of someone hand-writing these digits, the understanding the sequence of points might make much more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
